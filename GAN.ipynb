{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (5.4.1)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Collecting lxml>=3.6.0\n",
      "  Downloading lxml-4.6.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (2.8.1)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Collecting beautifulsoup4>=4.4.1\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tldextract>=2.0.1\n",
      "  Downloading tldextract-3.1.0-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cssselect>=0.9.2\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Pillow>=3.3.0\n",
      "  Downloading Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (3.4.5)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (2.25.1)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.2-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13537 sha256=f29940e580f6d5371e8b4014870bdf94d9449731438036bbac146ec9ef8369df\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/74/83/8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3354 sha256=a10a4dfbd150cd9da2589fff8f209bc4ffb7faacb3401b455a12b252a5f6a0c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/09/68/a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398406 sha256=97a6c9ad685546de860b75e86b050488bf69e8fe1628b39699a79b200694f24b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/7e/0c/54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=ad22637db20b6a79bc71a420a6323522ffcd3a8dbf973414724bd3674a642f4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: soupsieve, sgmllib3k, requests-file, filelock, beautifulsoup4, tldextract, tinysegmenter, Pillow, lxml, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
      "Successfully installed Pillow-8.1.2 beautifulsoup4-4.9.3 cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 filelock-3.0.12 jieba3k-0.35.1 lxml-4.6.2 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 soupsieve-2.2 tinysegmenter-0.3 tldextract-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.2.3-cp38-cp38-manylinux1_x86_64.whl (9.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.7 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.2.3 pytz-2021.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.19.4)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (8.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.4\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.4.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.3.17-cp38-cp38-manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp38-cp38-manylinux2010_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.56.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=0621f2a8b29356b8b158953905588f1edef43924530d7b893292e25fa571be45\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, joblib, tokenizers, sacremoses, transformers\n",
      "Successfully installed joblib-1.0.1 regex-2021.3.17 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nltk) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.0.8)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (7.19.0)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.4.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.0.5)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.11)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (4.7.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (52.0.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets) (2.7.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.8.1)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.7.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.2.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.8/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-7.6.3 jupyterlab-widgets-1.0.0 widgetsnbextension-3.5.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install sentencepiece\n",
    "!pip install ipywidgets\n",
    "# using nvidia-docker image from https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow/tags\n",
    "# using release 21.02-tf2-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>q3</th>\n",
       "      <th>perceived</th>\n",
       "      <th>primary.topic</th>\n",
       "      <th>secondary.topic</th>\n",
       "      <th>democrat.vote</th>\n",
       "      <th>republican.vote</th>\n",
       "      <th>article</th>\n",
       "      <th>length</th>\n",
       "      <th>democrat.bias</th>\n",
       "      <th>republican.bias</th>\n",
       "      <th>isbiased</th>\n",
       "      <th>biascategories</th>\n",
       "      <th>biasmagnitude</th>\n",
       "      <th>biasprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.usatoday.com/story/news/nation/2013...</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Maggie Clark  Pew Stateline Staff Writer  21 s...</td>\n",
       "      <td>702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.huffingtonpost.com/2013/05/21/senat...</td>\n",
       "      <td>News</td>\n",
       "      <td>0</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>WASHINGTON -- The Senate Judiciary Committee a...</td>\n",
       "      <td>1118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/dont-le...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>David Cole is a professor of constitutional la...</td>\n",
       "      <td>1401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.foxnews.com/politics/2013/04/03/oba...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Parts of ObamaCare are starting to fray, even ...</td>\n",
       "      <td>798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.breitbart.com/Big-Government/2013/1...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>U.S. Immigration Citizenship and Immigration S...</td>\n",
       "      <td>947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16584</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/dana-mi...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>President Obama won reelection in part by beat...</td>\n",
       "      <td>768</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16585</th>\n",
       "      <td>http://news.yahoo.com/republicans-hatred-obama...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>Red-faced Republicans, circling and preparing ...</td>\n",
       "      <td>805</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16586</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/dana-mi...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Rep. Steve Stockman’s moment as a viable Senat...</td>\n",
       "      <td>896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/ej-dion...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>We interrupt this highly partisan and ideologi...</td>\n",
       "      <td>751</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16588</th>\n",
       "      <td>http://www.huffingtonpost.com/2013/08/11/this-...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>FILE - In this Sept. 27, 2012 file photo, Dona...</td>\n",
       "      <td>378</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13193 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url       q3  perceived  \\\n",
       "0      http://www.usatoday.com/story/news/nation/2013...    other          1   \n",
       "1      http://www.huffingtonpost.com/2013/05/21/senat...     News          0   \n",
       "2      http://www.washingtonpost.com/opinions/dont-le...  Opinion          1   \n",
       "3      http://www.foxnews.com/politics/2013/04/03/oba...  Opinion          1   \n",
       "4      http://www.breitbart.com/Big-Government/2013/1...  Opinion          1   \n",
       "...                                                  ...      ...        ...   \n",
       "16584  http://www.washingtonpost.com/opinions/dana-mi...     News          1   \n",
       "16585  http://news.yahoo.com/republicans-hatred-obama...  Opinion          1   \n",
       "16586  http://www.washingtonpost.com/opinions/dana-mi...     News          1   \n",
       "16587  http://www.washingtonpost.com/opinions/ej-dion...     News          1   \n",
       "16588  http://www.huffingtonpost.com/2013/08/11/this-...  Opinion          1   \n",
       "\n",
       "             primary.topic      secondary.topic     democrat.vote  \\\n",
       "0             Civil Rights         Civil Rights           Neutral   \n",
       "1             Civil Rights         Civil Rights           Neutral   \n",
       "2             Civil Rights         Civil Rights           Neutral   \n",
       "3             Civil Rights         Civil Rights           Neutral   \n",
       "4             Civil Rights         Civil Rights           Neutral   \n",
       "...                    ...                  ...               ...   \n",
       "16584  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "16585  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "16586  Republican Scandals  Republican Scandals           Neutral   \n",
       "16587  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "16588  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "\n",
       "        republican.vote                                            article  \\\n",
       "0               Neutral  Maggie Clark  Pew Stateline Staff Writer  21 s...   \n",
       "1               Neutral  WASHINGTON -- The Senate Judiciary Committee a...   \n",
       "2               Neutral  David Cole is a professor of constitutional la...   \n",
       "3               Neutral  Parts of ObamaCare are starting to fray, even ...   \n",
       "4               Neutral  U.S. Immigration Citizenship and Immigration S...   \n",
       "...                 ...                                                ...   \n",
       "16584  SomewhatNegative  President Obama won reelection in part by beat...   \n",
       "16585  SomewhatNegative  Red-faced Republicans, circling and preparing ...   \n",
       "16586           Neutral  Rep. Steve Stockman’s moment as a viable Senat...   \n",
       "16587  SomewhatNegative  We interrupt this highly partisan and ideologi...   \n",
       "16588  SomewhatNegative  FILE - In this Sept. 27, 2012 file photo, Dona...   \n",
       "\n",
       "       length  democrat.bias  republican.bias  isbiased  biascategories  \\\n",
       "0         702            0.0              0.0         0               0   \n",
       "1        1118            0.0              0.0         0               0   \n",
       "2        1401            0.0              0.0         0               0   \n",
       "3         798            0.0              0.0         0               0   \n",
       "4         947            0.0              0.0         0               0   \n",
       "...       ...            ...              ...       ...             ...   \n",
       "16584     768            0.5             -0.5         1               1   \n",
       "16585     805            0.5             -0.5         1               1   \n",
       "16586     896            0.0              0.0         0               0   \n",
       "16587     751            0.5             -0.5         1               1   \n",
       "16588     378            0.5             -0.5         1               1   \n",
       "\n",
       "       biasmagnitude  biasprob  \n",
       "0                0.0      0.50  \n",
       "1                0.0      0.50  \n",
       "2                0.0      0.50  \n",
       "3                0.0      0.50  \n",
       "4                0.0      0.50  \n",
       "...              ...       ...  \n",
       "16584            1.0      0.75  \n",
       "16585            1.0      0.75  \n",
       "16586            0.0      0.50  \n",
       "16587            1.0      0.75  \n",
       "16588            1.0      0.75  \n",
       "\n",
       "[13193 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table(\"biaseddata.csv\", sep='\\t', index_col=0)\n",
    "# data = pd.read_table(\"processedbiasdata.csv\", sep='\\t', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13193, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "t5_model = 't5-small'\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(t5_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "sample_text = data.article[1]\n",
    "encoding = t5_tokenizer.encode(\"\"\"summarize: \"\"\" + sample_text, return_tensors='tf', max_length=1000)\n",
    "\n",
    "\n",
    "outputs = model.generate(encoding,\n",
    "                      num_beams=4, \n",
    "                      no_repeat_ngram_size=2,\n",
    "                      min_length=100,\n",
    "                      max_length=1000,\n",
    "                      early_stopping=True)\n",
    "\n",
    "summarization = t5_tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> the gang of eight immigration bill passed in a 13 to 5 vote on monday. the bill will now go to the Senate floor, but all Democrats voted in favor of it, sen. john cornyn said if it had been between his vote and moving it forward, it would have been on the floor of the senator\\'s committee he would support it. \"the dysfunction in our current immigration system affects all of us and it is long past time for reform,\" Judiciary Chairman Patrick Leah'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON -- The Senate Judiciary Committee approved the bipartisan \"gang of eight\" immigration bill on Tuesday in a 13 to 5 vote after a marathon final day of markup that stretched into the evening.  All Democrats on the committee, along with Sen. Orrin Hatch (R-Utah) and gang of eight Republican Sens. Lindsey Graham (R-S.C.) and Jeff Flake (R-Ariz.), voted in favor of the bill, which will now go to the Senate floor. Sen. John Cornyn (R-Texas), who voted against the bill out of committee, said he would support allowing it to move forward for debate -- rather than joining a filibuster -- once on the Senate floor. Sen. Chuck Grassley (R-Iowa), who was also a \"no\" vote, said if it had been between his vote and moving the bill to the Senate floor, he would have voted in favor.  The crowd in the room erupted into applause when the final vote tally was read, rising to their feet and chanting \"Yes we can!\" then \"Si se puede!\"  Democrats seemed equally pleased to vote the bill out of committee.  \"The dysfunction in our current immigration system affects all of us and it is long past time for reform\" Judiciary Chairman Patrick Leahy (D-Vt.) said before the bill\\'s passage. \"I hope that our history our values and our decency can inspire us finally to take action. We need an immigration system that lives up to American values and helps write the next great chapter in American history by reinvigorating our economy and enriching our communities.\"  Of 300 amendments offered to the gang of eight immigration bill, the committee debated more than 200.  The bill came out of months of work from the gang of eight, which in addition to Graham and Flake includes Sens. John McCain (R-Ariz.), Marco Rubio (R-Fla.), Chuck Schumer (D-N.Y.), Dick Durbin (D-Ill.), Michael Bennet (D-Colo.) and Bob Menendez (D-N.J.). It includes a path to citizenship for undocumented immigrants, a streamlined legal immigration process, tougher interior enforcement and increased border security.  One of the final issues to settle was over high-skilled worker visas called H1-Bs. An amendment that simplifies the process for companies to bring in workers in science, technology, engineering and math eventually passed on a voice vote. Hatch and Schumer came to a compromise that led Hatch to agree to vote the full bill out of committee, although he did not commit to supporting its eventual passage.  It was a tough vote for Sen. Dick Durbin (D-Ill.), who has previously been wary of expanding H1-B visas. He voted in favor, however, citing the need for compromise and to get the bill through.  \"We\\'ve made concessions I never thought we\\'d have to make ... but we\\'ve made those concessions to win your support\" Durbin said to Hatch. \"We need your support and we want to pass this bill.\"  The amendment came under fire from the AFL-CIO labor federation, which said the measure would hurt American workers.  \"There is no reason why this strong coalition should accept anti-worker amendments\" AFL-CIO President Richard Trumka said in a statement. \"And let\\'s be clear: Senator Orrin Hatch\\'s H-1B amendments are unambiguous attacks on American workers.\"  Some measures praised highly by immigrant rights advocates never went to a vote. Leahy brought up his amendment to allow LGBT couples equal protection under immigration law, but withdrew it \"with a heavy heart\" when it became clear Democrats were willing to drop it so the bill would pass.  A number of provisions lauded by advocates did make it in. The committee voted on Monday to approve Blumenthal\\'s amendment that would block immigration raids in sensitive areas such as schools, hospitals and places of worship, except in extreme circumstances. A separate Blumenthal measure to restrict the use of solitary confinement in immigrant detention also passed on Monday, also on a voice vote.  Although many Republican amendments failed, some GOP senators, such as Hatch, were more successful than others. The committee voted on Monday to require a mandatory biometric exit system at airports with the highest volume of international air travel -- something Sen. Jeff Sessions (R-Ala.) wanted to do nationwide but without success.  Amendments deemed poison pills failed, unsurprisingly. Sen. Ted Cruz (R-Texas) attempted to add a change that would strip the bill of its path to citizenship for undocumented immigrants, which is considered absolutely necessary to the bill by President Barack Obama, Democrats and some Republicans. Five Republicans voted for that amendment, while three -- Hatch, Graham and Flake -- joined Democrats in opposing it.  Leahy said the amendment would gut the bill, while Sen. Chuck Schumer (D-N.Y.) said \"it goes against everything America stands for.\" Sen. Dick Durbin (D-Ill.) similarly disparaged another failed Cruz amendment, this one to bar currently undocumented immigrants from ever being eligible for welfare benefits such as Medicaid and food stamps, saying he doesn\\'t think such an America is one he\\'d want to live in.  Sessions was also unsuccessful in his efforts to amend the bill, offering 49 amendments and seeing nearly every one that went for a vote fail. On one amendment last week Sessions was the sole \"yes\" vote, with all of his Republican colleagues abandoning him in his effort to limit legal immigration.  Sen. Mazie Hirono (D-Hawaii) was one of the rare Democrats to offer an amendment that the gang of eight took down. Schumer and Durbin told her they had to reluctantly oppose her attempt on Tuesday to add a change that would expand family visas so that people in extreme hardship could petition for green cards for their adult children or siblings. It failed with 7 votes in support and 11 against, including Democrats Schumer, Durbin, and Sen. Dianne Feinstein (D-Calif.), all of whom said it was a good amendment but would upset the delicate balance of the gang of eight bill.  \"Your heart\\'s in the right place your amendment\\'s in the wrong place\" Durbin said to Hirono.  President Barack Obama applauded the committee for passing the bill.  \"None of the Committee members got everything they wanted and neither did I but in the end we all owe it to the American people to get the best possible result over the finish line\" he said in a statement. \"I encourage the full Senate to bring this bipartisan bill to the floor at the earliest possible opportunity and remain hopeful that the amendment process will lead to further improvements.\"  Now that the bill passed the Senate Judiciary Committee, it has to go to the Senate floor, where gang of eight members are hopeful it will pass. Schumer and McCain, who is not on the Judiciary Committee, have said they would like to win over a majority of both parties -- around 70 votes in total.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Gan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>q3</th>\n",
       "      <th>perceived</th>\n",
       "      <th>primary.topic</th>\n",
       "      <th>secondary.topic</th>\n",
       "      <th>democrat.vote</th>\n",
       "      <th>republican.vote</th>\n",
       "      <th>article</th>\n",
       "      <th>length</th>\n",
       "      <th>democrat.bias</th>\n",
       "      <th>republican.bias</th>\n",
       "      <th>isbiased</th>\n",
       "      <th>biascategories</th>\n",
       "      <th>biasmagnitude</th>\n",
       "      <th>biasprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.usatoday.com/story/news/nation/2013...</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Maggie Clark  Pew Stateline Staff Writer  21 s...</td>\n",
       "      <td>702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.huffingtonpost.com/2013/05/21/senat...</td>\n",
       "      <td>News</td>\n",
       "      <td>0</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>WASHINGTON -- The Senate Judiciary Committee a...</td>\n",
       "      <td>1118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/dont-le...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>David Cole is a professor of constitutional la...</td>\n",
       "      <td>1401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.foxnews.com/politics/2013/04/03/oba...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Parts of ObamaCare are starting to fray, even ...</td>\n",
       "      <td>798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.breitbart.com/Big-Government/2013/1...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Civil Rights</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>U.S. Immigration Citizenship and Immigration S...</td>\n",
       "      <td>947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13188</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/dana-mi...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>President Obama won reelection in part by beat...</td>\n",
       "      <td>768</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13189</th>\n",
       "      <td>http://news.yahoo.com/republicans-hatred-obama...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>Red-faced Republicans, circling and preparing ...</td>\n",
       "      <td>805</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13190</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/dana-mi...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Rep. Steve Stockman’s moment as a viable Senat...</td>\n",
       "      <td>896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13191</th>\n",
       "      <td>http://www.washingtonpost.com/opinions/ej-dion...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>We interrupt this highly partisan and ideologi...</td>\n",
       "      <td>751</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13192</th>\n",
       "      <td>http://www.huffingtonpost.com/2013/08/11/this-...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>Republican Scandals</td>\n",
       "      <td>SomewhatPositive</td>\n",
       "      <td>SomewhatNegative</td>\n",
       "      <td>FILE - In this Sept. 27, 2012 file photo, Dona...</td>\n",
       "      <td>378</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13193 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url       q3  perceived  \\\n",
       "0      http://www.usatoday.com/story/news/nation/2013...    other          1   \n",
       "1      http://www.huffingtonpost.com/2013/05/21/senat...     News          0   \n",
       "2      http://www.washingtonpost.com/opinions/dont-le...  Opinion          1   \n",
       "3      http://www.foxnews.com/politics/2013/04/03/oba...  Opinion          1   \n",
       "4      http://www.breitbart.com/Big-Government/2013/1...  Opinion          1   \n",
       "...                                                  ...      ...        ...   \n",
       "13188  http://www.washingtonpost.com/opinions/dana-mi...     News          1   \n",
       "13189  http://news.yahoo.com/republicans-hatred-obama...  Opinion          1   \n",
       "13190  http://www.washingtonpost.com/opinions/dana-mi...     News          1   \n",
       "13191  http://www.washingtonpost.com/opinions/ej-dion...     News          1   \n",
       "13192  http://www.huffingtonpost.com/2013/08/11/this-...  Opinion          1   \n",
       "\n",
       "             primary.topic      secondary.topic     democrat.vote  \\\n",
       "0             Civil Rights         Civil Rights           Neutral   \n",
       "1             Civil Rights         Civil Rights           Neutral   \n",
       "2             Civil Rights         Civil Rights           Neutral   \n",
       "3             Civil Rights         Civil Rights           Neutral   \n",
       "4             Civil Rights         Civil Rights           Neutral   \n",
       "...                    ...                  ...               ...   \n",
       "13188  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "13189  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "13190  Republican Scandals  Republican Scandals           Neutral   \n",
       "13191  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "13192  Republican Scandals  Republican Scandals  SomewhatPositive   \n",
       "\n",
       "        republican.vote                                            article  \\\n",
       "0               Neutral  Maggie Clark  Pew Stateline Staff Writer  21 s...   \n",
       "1               Neutral  WASHINGTON -- The Senate Judiciary Committee a...   \n",
       "2               Neutral  David Cole is a professor of constitutional la...   \n",
       "3               Neutral  Parts of ObamaCare are starting to fray, even ...   \n",
       "4               Neutral  U.S. Immigration Citizenship and Immigration S...   \n",
       "...                 ...                                                ...   \n",
       "13188  SomewhatNegative  President Obama won reelection in part by beat...   \n",
       "13189  SomewhatNegative  Red-faced Republicans, circling and preparing ...   \n",
       "13190           Neutral  Rep. Steve Stockman’s moment as a viable Senat...   \n",
       "13191  SomewhatNegative  We interrupt this highly partisan and ideologi...   \n",
       "13192  SomewhatNegative  FILE - In this Sept. 27, 2012 file photo, Dona...   \n",
       "\n",
       "       length  democrat.bias  republican.bias  isbiased  biascategories  \\\n",
       "0         702            0.0              0.0         0               0   \n",
       "1        1118            0.0              0.0         0               0   \n",
       "2        1401            0.0              0.0         0               0   \n",
       "3         798            0.0              0.0         0               0   \n",
       "4         947            0.0              0.0         0               0   \n",
       "...       ...            ...              ...       ...             ...   \n",
       "13188     768            0.5             -0.5         1               1   \n",
       "13189     805            0.5             -0.5         1               1   \n",
       "13190     896            0.0              0.0         0               0   \n",
       "13191     751            0.5             -0.5         1               1   \n",
       "13192     378            0.5             -0.5         1               1   \n",
       "\n",
       "       biasmagnitude  biasprob  \n",
       "0                0.0      0.50  \n",
       "1                0.0      0.50  \n",
       "2                0.0      0.50  \n",
       "3                0.0      0.50  \n",
       "4                0.0      0.50  \n",
       "...              ...       ...  \n",
       "13188            1.0      0.75  \n",
       "13189            1.0      0.75  \n",
       "13190            0.0      0.50  \n",
       "13191            1.0      0.75  \n",
       "13192            1.0      0.75  \n",
       "\n",
       "[13193 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "# data = data.sample(frac=1)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13193, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b0162bcf7b94>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfdf['text'] = data['article']\n",
      "<ipython-input-15-b0162bcf7b94>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfdf['label'] = data['isbiased']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the rec list! Thank you all so much! Keep s...</td>\n",
       "      <td>biased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 years ago  (CNN) - For the first time in his...</td>\n",
       "      <td>unbiased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What I'm about to suggest works. I know it doe...</td>\n",
       "      <td>biased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gregory Korte, USA TODAY  Incoming president s...</td>\n",
       "      <td>unbiased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Story highlights It has been two years since O...</td>\n",
       "      <td>unbiased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  On the rec list! Thank you all so much! Keep s...    biased\n",
       "1  8 years ago  (CNN) - For the first time in his...  unbiased\n",
       "2  What I'm about to suggest works. I know it doe...    biased\n",
       "3  Gregory Korte, USA TODAY  Incoming president s...  unbiased\n",
       "4  Story highlights It has been two years since O...  unbiased"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf = data[['article','isbiased']]\n",
    "tfdf['text'] = data['article']\n",
    "tfdf['label'] = data['isbiased']\n",
    "tfdf = tfdf[['text','label']]\n",
    "tfdf['label'] = tfdf['label'].replace(0, \"unbiased\")\n",
    "tfdf['label'] = tfdf['label'].replace(1, \"biased\")\n",
    "tfdf = tfdf.sample(frac=1)\n",
    "tfdf = tfdf.reset_index(drop=True)\n",
    "\n",
    "print(tfdf.shape)\n",
    "tfdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdf[\"label\"][0:10000].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdf[\"label\"][10000:].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(data.length[0:10000], bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfdf[:10000].to_dict('records')\n",
    "test_data = tfdf[10000:].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tfdata = tf.data.Dataset.from_tensor_slices(train_data[:3])\n",
    "# # test_tfdata = tf.data.Dataset.from_tensor_slices(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larget5_model = 't5-large'\n",
    "\n",
    "# larget5_tokenizer = T5Tokenizer.from_pretrained(larget5_model)\n",
    "# largemodel = TFT5ForConditionalGeneration.from_pretrained(larget5_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data[\"isbiased\"]==1][\"article\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps:  1000\n",
      "Total Validation Steps:  320\n"
     ]
    }
   ],
   "source": [
    "warmup_steps = 1000\n",
    "batch_size = 10\n",
    "encoder_max_len = 2000\n",
    "\n",
    "discriminator_max_len = 8 \n",
    "generator_max_len = 1000\n",
    "# generator_min_len = 500\n",
    "\n",
    "buffer_size = 5000\n",
    "ntrain = len(train_data)\n",
    "nvalid = len(test_data)\n",
    "steps = int(np.ceil(ntrain/batch_size))\n",
    "valid_steps = int(np.ceil(nvalid/batch_size))\n",
    "print(\"Total Steps: \", steps)\n",
    "print(\"Total Validation Steps: \", valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "log_dir = f\"{data_dir}/experiments/t5/logs\"\n",
    "save_path = f\"{data_dir}/experiments/t5/models\"\n",
    "cache_path_train = f\"{data_dir}/cache/t5.train\"\n",
    "cache_path_test = f\"{data_dir}/cache/t5.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\",  use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the rec list! Thank you all so much! Keep s...</td>\n",
       "      <td>biased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 years ago  (CNN) - For the first time in his...</td>\n",
       "      <td>unbiased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What I'm about to suggest works. I know it doe...</td>\n",
       "      <td>biased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gregory Korte, USA TODAY  Incoming president s...</td>\n",
       "      <td>unbiased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Story highlights It has been two years since O...</td>\n",
       "      <td>unbiased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  On the rec list! Thank you all so much! Keep s...    biased\n",
       "1  8 years ago  (CNN) - For the first time in his...  unbiased\n",
       "2  What I'm about to suggest works. I know it doe...    biased\n",
       "3  Gregory Korte, USA TODAY  Incoming president s...  unbiased\n",
       "4  Story highlights It has been two years since O...  unbiased"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(isinstance(x, str) for x in list(tfdf.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(isinstance(x, str) for x in list(tfdf.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_section = tfdf[:10000]\n",
    "test_section = tfdf[10000:]\n",
    "train_dataset =  tf.data.Dataset.from_tensor_slices((list(train_section['text']), list(train_section['label'])))\n",
    "test_dataset =  tf.data.Dataset.from_tensor_slices((list(test_section['text']), list(test_section['label'])))\n",
    "# train_section = tfdf[:10000].to_dict('records')\n",
    "# test_section = tfdf[10000:].to_dict('records')\n",
    "# train_section = tfdf[:10000].to_dict('list')\n",
    "# test_section = tfdf[10000:].to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf[:10000].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def meta_dict_gen(d):\n",
    "#     for i in range(d.shape[]):\n",
    "#         ls = {}\n",
    "#         for key, val in d.items():\n",
    "#             ls[key] = val[i]\n",
    "#         yield ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_generator(\n",
    "#     meta_dict_gen,\n",
    "#     output_types={k: tf.float32 for k in metadata},\n",
    "#     output_shapes={'m1': (2,), 'm2': (3, 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset =  tf.data.Dataset.from_tensor_slices((train_section))\n",
    "# test_dataset =  tf.data.Dataset.from_tensor_slices((test_section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'On the rec list! Thank you all so much! Keep spreading the word about Senator Begich\\'s plan. He has the right idea about what to do to strengthen Social Security and we need to get Democrats to make this the party\\'s official plan. Expect more diaries about Begich from me.  Ok, so far Laurence Lewis\\' front page post is about Chuck Hagel and John Brennan, which yes highlights some valid points and the rec list has diaries about Obama wanting to cut Social Security. Great way to start Sunday Morning off with the first big Obama Rocks Obama Sucks Pie Fight. Well since the rec list gets filled with both \"Obama Will Betray Us\" and \"11th Dimensional Chess\" themed diaries, I for one am going to repeat a call for action that I have made several times but only made the rec list once. And that is to get everyone to get their Senator and Congressman to back Senator Mark Begich\\'s (D. AK) plan to strengthen Social Security. I\\'ve already written about it quite a few times but if Sunday is going to turn out to be a big pie fight, I will take the role of the diarist actually proposing solutions.  Ok, so here\\'s the very first diary I wrote about Begich\\'s plan to strengthen Social Security:  http:  www.dailykos.com ...  Begich\\'s plan is known as the Protecting and Preserving Social Security Act and here\\'s what the plan calls for:    Lifts the Cap on High-Income Contributions. Current law sets a cap based on income at $113,700 for paying into Social Security. If an individual\\xe2\\x80\\x99s wages hit that total for the year, they no longer pay into the program. Sen. Begich\\xe2\\x80\\x99s bill lifts the cap and asks higher income earners to pay Social Security on all their earnings in order to increase the program\\xe2\\x80\\x99s revenue stream and extend the overall solvency of the program. Extends Social Security for approximately 75 years through modest revenue increases gradually implemented over the course of seven years. - Alaska Native News, 11 14 12  \\xe2\\x80\\x9cSen. Begich\\xe2\\x80\\x99s bill serves the American people well in very important ways,\\xe2\\x80\\x9d said Eric Kingson, co-chair of the Strengthen Social Security Coalition. \\xe2\\x80\\x9cBy asking high-income people to pay the same payroll tax contribution as everyone else, he dramatically improves the financing of Social Security. By improving the accuracy of the cost of living adjustment, he assures that seniors and people with disabilities will be able to maintain their standard of living as time goes on.\\xe2\\x80\\x9d - Alaska Native News, 11 14 12  \"Mr. President a few weeks ago back home in Anchorage I joined a group of seniors I presented this piece of legislation to them at the Anchorage Senior Center and she says she loves to describe herself as a \"young woman from Alaska\" stood up. Beverly Moore an 81-year old Korean War Navy veteran. Beverly was there because the majority of her modest income comes from Social Security. And she wanted to know how this proposal will strengthen that lifeline for her and thousands of Alaskans. In fact one in nine Alaskans receive Social Security. With my states population of those 65 and older expanding rapidly Social Security will continue to play a key role in supplementing a decent living. If Social Security was not there for the elderly Alaskans a fifth of them would live below poverty. It\\'s vital for our state it\\'s vital for all our states and for this whole country. Mr. President I have no illusions that this bill is going to pass in the final weeks of this 112th congress but I wanted to get it into the mix. I wanted to make sure people got the bigger point and again I would say to my residing officer and says this well and I know my friend here from Oregon who is on the floor also as we talk about the deficit that has taken center stage right now we want to highlight one very clear thing: Social Security has not contributed is not part of and never will contribute to the deficit. So those who like to meddle in it and try to combine it into this deficit talk are just playing games with our seniors and disabled in this country.\" - U.S. Senator Mark Begich (D. AK), 12 7 12  You can help in three ways. (1) Call Senator Begich\\xe2\\x80\\x99s office and thank him. It\\xe2\\x80\\x99s seriously important to let him know that real progressives are behind a real progressive proposal. His DC office number is: Senator Mark Begich  (202) 224 \\xe2\\x80\\x93 3004  (877) 501 \\xe2\\x80\\x93 6275 (toll-free) (2) Get behind this bill. If you can publicize it, do. If you can write about it, do. If you can talk about it, do. We need to do our part as well to promote real progressive legislation. The next four years should not be a spectator sport. (3) Call your senators \\xe2\\x80\\x94 both of them \\xe2\\x80\\x94 and ask them to co-sign this bill. This needs momentum, and co-signers provide that. The list of Senate phone numbers is here.: http:  www.senate.gov ... - America Blog, 11 19 12  Now here\\'s the actual plan:The Alaskan AARP and the coalition to strengthen Social Security have endorsed Begich\\'s plan:Now here\\'s Begich making the case for the Protecting and Preserving Social Security Act on the Senate floor and explaining how Social Security doesn\\'t contribute to the defict:Sounds like a great plan, right? Wouldn\\'t it be nice if every Democratic Senator backed this plan? Well instead of spending the day fighting each other in the comments section of people\\'s diaries, how about we focus on putting the pressure on the Senate to make it a reality. Here\\'s how you can do so:I\\'ll leave you with this diary. Every time I sense an Obama pie fight regarding Social Security a brewing, I\\'m just going to post another diary about Begich\\'s plan. Personally, I like inspiring you guys to take action with actual solutions rather than argue with you. Happy Sunday!'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'biased'>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataset))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(context, label, encoder_max_len=encoder_max_len, \n",
    "           gen_decoder_max_len=generator_max_len, dis_decoder_max_len = discriminator_max_len):\n",
    "#     print(context)\n",
    "#     print(label)\n",
    "    global c\n",
    "    c+=1\n",
    "    print(c)\n",
    "#     print(\"context: \"+context)\n",
    "#     print(\"label: \"+label)\n",
    "    context_plus = f\"context:  {str(context.numpy().decode('utf-8'))} </s>\"\n",
    "    text_plus = f\"paraphrase:   {str(context.numpy().decode('utf-8'))} </s>\"\n",
    "    label_plus = f\"label:   {str(context.numpy().decode('utf-8'))} </s>\"\n",
    "\n",
    "    encoder_inputs = tokenizer(context_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=encoder_max_len,\n",
    "                              padding=True)\n",
    "    \n",
    "    gener_decoder_inputs = tokenizer(text_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=gen_decoder_max_len,\n",
    "                              padding=True)\n",
    "    \n",
    "    discr_decoder_inputs = tokenizer(label_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=dis_decoder_max_len,\n",
    "                              padding=True)\n",
    "    \n",
    "    input_ids = encoder_inputs['input_ids'][0]\n",
    "    input_attention = encoder_inputs['attention_mask'][0]\n",
    "    \n",
    "    gener_target_ids = gener_decoder_inputs['input_ids'][0]\n",
    "    gener_target_attention = gener_decoder_inputs['attention_mask'][0]\n",
    "    \n",
    "    discr_target_ids = discr_decoder_inputs['input_ids'][0]\n",
    "    discr_target_attention = discr_decoder_inputs['attention_mask'][0]\n",
    "\n",
    "    return input_ids, input_attention, gener_target_ids, gener_target_attention, discr_target_ids, discr_target_attention\n",
    "#     outputs = {'input_ids':input_ids, 'attention_mask': input_attention, \n",
    "#                'generator_ids':gener_target_ids, 'generator_attention_mask':gener_target_attention,\n",
    "#                'discriminator_ids':discr_target_ids, 'discriminator_attention_mask':discr_target_attention}\n",
    "# #     return input_ids,input_attention, gener_target_ids, gener_target_attention, discr_target_ids, discr_target_attention\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_tf(inputs):\n",
    "def encode_tf(text, label):\n",
    "#     context = inputs['context']\n",
    "#     question = inputs['question']\n",
    "#     answer = inputs['answers']['text']\n",
    "    encoded = tf.py_function(encode, [text, label], \n",
    "                                           [tf.int32, tf.int32, tf.int32, tf.int32,tf.int32, tf.int32])\n",
    "    input_ids,input_attention, generator_ids, generator_attention, discriminator_ids, discriminator_attention = encoded\n",
    "    \n",
    "    input_ids.set_shape([None])\n",
    "    generator_ids.set_shape([None])\n",
    "    discriminator_ids.set_shape([None])\n",
    "    \n",
    "    input_attention.set_shape([None])\n",
    "    generator_attention.set_shape([None])\n",
    "    discriminator_attention.set_shape([None])\n",
    "    \n",
    "#     labels = tf.reshape(target_ids, [-1, 1])\n",
    "    data=  {'input_ids': input_ids, #'decoder_input_ids': target_ids, \n",
    "            'generator_ids': generator_ids,\n",
    "            'discriminator_ids': discriminator_ids,\n",
    "            'input_attention_mask': input_attention,\n",
    "            'generator_attention_mask': generator_attention,\n",
    "           'discriminator_attention_mask': discriminator_attention}\n",
    "    return (data, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(source_dataset, cache_path=None, batch_size=4, \n",
    "                   buffer_size= 1000, shuffling=True):\n",
    "#     dataset = source_dataset.map(encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = source_dataset.map(encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    if cache_path is not None:\n",
    "        dataset = dataset.cache(cache_path)        \n",
    "    if shuffling:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_dataset.map(encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf.shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_tf))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds= create_dataset(train_dataset, batch_size=batch_size, \n",
    "                         shuffling=True, cache_path = None)\n",
    "test_ds = create_dataset(test_dataset, batch_size=batch_size, \n",
    "                         shuffling=False, cache_path = None)\n",
    "# https://stackoverflow.com/questions/49531286/tensorflow-tf-data-dataset-cannot-batch-tensors-with-different-shapes-in-compo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:174: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "2021\n",
      "22\n",
      "\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "2829\n",
      "\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "4344\n",
      "\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "7980\n",
      "81\n",
      "82\n",
      "83\n",
      "\n",
      "8485\n",
      "\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99100\n",
      "\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306307\n",
      "308\n",
      "\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315316\n",
      "317\n",
      "318\n",
      "\n",
      "319320\n",
      "321\n",
      "322\n",
      "323324\n",
      "325\n",
      "\n",
      "326\n",
      "\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345346347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "\n",
      "\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496497\n",
      "\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551552\n",
      "\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565566\n",
      "567\n",
      "568\n",
      "\n",
      "569\n",
      "570\n",
      "571572\n",
      "573\n",
      "\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601602\n",
      "\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621622\n",
      "\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636637\n",
      "\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669670\n",
      "\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679680\n",
      "681\n",
      "682\n",
      "\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762763\n",
      "\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776777\n",
      "778\n",
      "779\n",
      "\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813814\n",
      "815\n",
      "816\n",
      "817\n",
      "\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832833\n",
      "834\n",
      "\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866867\n",
      "\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878879\n",
      "\n",
      "880\n",
      "881\n",
      "882\n",
      "883884\n",
      "885\n",
      "886\n",
      "\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940941\n",
      "942\n",
      "943\n",
      "\n",
      "944\n",
      "945\n",
      "946947\n",
      "\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998999\n",
      "1000\n",
      "1001\n",
      "\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "10081009\n",
      "1010\n",
      "\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot batch tensors with different shapes in component 2. First element had shape [991] and element 1 had shape [745].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 2. First element had shape [991] and element 1 had shape [745]. [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c2114241dc4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 2. First element had shape [991] and element 1 had shape [745]."
     ]
    }
   ],
   "source": [
    "data = next(iter(train_ds))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = [encode(i) for i in train_data]\n",
    "# test_ds = [encode(i) for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tfdataset = tf.data.Dataset.from_tensor_slices(train_ds[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(train_ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(source_dataset, cache_path=None, batch_size=4, \n",
    "#                    buffer_size= 1000, shuffling=True):\n",
    "#     dataset = source_dataset.map(encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "#     if cache_path is not None:\n",
    "#         dataset = dataset.cache(cache_path)        \n",
    "#     if shuffling:\n",
    "#         dataset = dataset.shuffle(buffer_size)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def to_tf_dataset(dataset):  \n",
    "#   columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
    "#   dataset.set_format(type='tensorflow', columns=columns)\n",
    "#   return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \n",
    "#                 'labels':tf.int32, 'decoder_attention_mask':tf.int32,  }\n",
    "#   return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \n",
    "#                   'labels': tf.TensorShape([None]), 'decoder_attention_mask':tf.TensorShape([None])}\n",
    "#   ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\n",
    "#   return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_dataset(dataset):  \n",
    "    columns = ['input_ids', 'attention_mask', 'generator_ids', 'generator_attention_mask','discriminator_ids','discriminator_attention_mask']\n",
    "    dataset.set_format(type='tensorflow', columns=columns)\n",
    "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \n",
    "                'generator_ids':tf.int32, 'generator_attention_mask':tf.int32,\n",
    "                   'discriminator_ids':tf.int32, 'discriminator_attention_mask':tf.int32,  }\n",
    "    \n",
    "    return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \n",
    "                  'generator_ids': tf.TensorShape([None]), 'generator_attention_mask':tf.TensorShape([None]),\n",
    "                    'discriminator_ids': tf.TensorShape([None]), 'discriminator_attention_mask':tf.TensorShape([None]),}\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_ds = to_tf_dataset(train_ds)\n",
    "tf_test_ds = to_tf_dataset(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_text = data.article[8]\n",
    "encoding = larget5_tokenizer.encode(\"\"\"summarize: \"\"\" + sample_text, return_tensors='tf', max_length=1000)\n",
    "\n",
    "\n",
    "outputs = largemodel.generate(encoding,\n",
    "                      num_beams=4, \n",
    "                      no_repeat_ngram_size=2,\n",
    "                      min_length=500,\n",
    "                      max_length=1000,\n",
    "                      early_stopping=True)\n",
    "\n",
    "summarization = larget5_tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_text = data.article[8]\n",
    "encoding = larget5_tokenizer.encode(\"\"\"summarize: \"\"\" + sample_text, return_tensors='tf', max_length=1000)\n",
    "\n",
    "\n",
    "outputs = largemodel.generate(encoding,\n",
    "                      num_beams=4, \n",
    "                      no_repeat_ngram_size=2,\n",
    "                      min_length=500,\n",
    "                      max_length=1000,\n",
    "                      early_stopping=True)\n",
    "\n",
    "summarization = larget5_tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "\n",
    "# task = \"summarize: \"\n",
    "task = \"paraphrase: \"\n",
    "\n",
    "t5_train_text = [task+i for i in train_text]\n",
    "t5_test_text = [task+ i for i in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from T5 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminatortT5(TFT5ForConditionalGeneration):\n",
    "    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_tracker= tf.keras.metrics.Mean(name='loss') \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        y = x[\"labels\"]\n",
    "        y = tf.reshape(y, [-1, 1])\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(x, training=True)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "            \n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        lr = self.optimizer._decayed_lr(tf.float32)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)        \n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        metrics = {m.name: m.result() for m in self.metrics}\n",
    "        metrics.update({'lr': lr})\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        y = x[\"labels\"]\n",
    "        y = tf.reshape(y, [-1, 1])\n",
    "        output = self(x, training=False)\n",
    "        loss = output[0]\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        logits = output[1]\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 1e4\n",
    "batch_size = 4\n",
    "encoder_max_len = 250\n",
    "decoder_max_len = 54\n",
    "buffer_size = 1000\n",
    "ntrain = len(train_dataset)\n",
    "nvalid = len(valid_dataset)\n",
    "steps = int(np.ceil(ntrain/batch_size))\n",
    "valid_steps = int(np.ceil(nvalid/batch_size))\n",
    "print(\"Total Steps: \", steps)\n",
    "print(\"Total Validation Steps: \", valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example,\n",
    "           encoder_max_len=encoder_max_len, decoder_max_len=decoder_max_len):\n",
    "  \n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    answer = example['answers']['text']\n",
    "  \n",
    "    question_plus = f\"answer_me: {str(question)}\"\n",
    "    question_plus += f\" context: {str(context)} </s>\"\n",
    "    \n",
    "    answer_plus = ', '.join([i for i in list(answer)])\n",
    "    answer_plus = f\"{answer_plus} </s>\"\n",
    "    \n",
    "    encoder_inputs = tokenizer(question_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=encoder_max_len,\n",
    "                              pad_to_max_length=True)\n",
    "    \n",
    "    decoder_inputs = tokenizer(answer_plus, truncation=True, \n",
    "                               return_tensors='tf', max_length=decoder_max_len,\n",
    "                              pad_to_max_length=True)\n",
    "    \n",
    "    input_ids = encoder_inputs['input_ids'][0]\n",
    "    input_attention = encoder_inputs['attention_mask'][0]\n",
    "    target_ids = decoder_inputs['input_ids'][0]\n",
    "    target_attention = decoder_inputs['attention_mask'][0]\n",
    "    \n",
    "    outputs = {'input_ids':input_ids, 'attention_mask': input_attention, \n",
    "               'labels':target_ids, 'decoder_attention_mask':target_attention}\n",
    "    return outputs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=  train_dataset.map(encode)\n",
    "valid_ds=  valid_dataset.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def t5_keras_model():\n",
    "    \n",
    "    \n",
    "    encode_in = tf.keras.layers.Input(shape=(max_len,), dtype='int32', name=\"encode_in_ids\")\n",
    "    enc_mask_in = tf.keras.layers.Input(shape=(max_len,), dtype='int32', name=\"enc_mask_in_ids\")\n",
    "    decode_in = tf.keras.layers.Input(shape=(None,), dtype='int32', name=\"decode_in_ids\")\n",
    "    dec_mask_in = tf.keras.layers.Input(shape=(None,), dtype='int32', name=\"dec_mask_in_ids\")\n",
    "    \n",
    "    t5_layer = TFT5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "    \n",
    "    t5_out = t5_layer({'input_ids': encode_in, \n",
    "                       'decoder_input_ids':decode_in, \n",
    "                       'attention_mask':enc_mask_in,\n",
    "                       'decoder_attention_mask':dec_mask_in\n",
    "                      }, \n",
    "                             return_dict=True)\n",
    "    \n",
    "    pred_logits = t5_out['logits']\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[encode_in, \n",
    "                                          enc_mask_in, \n",
    "                                          decode_in,\n",
    "                                          dec_mask_in\n",
    "                                         ], \n",
    "                                  outputs=pred_logits)\n",
    "\n",
    "    model.compile(loss=t5_custom_loss, \n",
    "                  optimizer=tf.keras.optimizers.Adam(), \n",
    "                  metrics=[\n",
    "                  #     tf.keras.metrics.Accuracy(),\n",
    "                          t5_custom_acc_orig_tokens, \n",
    "                           t5_custom_acc_orig_tokens_no_other,\n",
    "                      t5_custom_acc_orig_tokens_begin_cont,\n",
    "                      t5_custom_acc_orig_tokens_not_begin_cont_other\n",
    "                  #\n",
    "                  ]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "SHUFFEL_SIZE = 1024\n",
    "\n",
    "learning_rate = 3e-5\n",
    "\n",
    "model_size = \"t5-small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_size)\n",
    "\n",
    "model = TFT5model.from_pretrained(model_size)\n",
    "\n",
    "task_specific_params = model.config.task_specific_params\n",
    "if task_specific_params is not None:\n",
    "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_specific_params = model.config.task_specific_params\n",
    "if task_specific_params is not None:\n",
    "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
    "    \n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "SHUFFEL_SIZE = 512\n",
    "\n",
    "learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08, clipnorm=1.0)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
